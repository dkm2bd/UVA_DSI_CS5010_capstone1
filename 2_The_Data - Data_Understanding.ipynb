{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS5010, Summer 2020 Capstone 1 Web Scraper\n",
    "Data Understanding </H3>\n",
    "    \n",
    "Data Features:\n",
    "\n",
    "___Initial Data Collection Report___: The Data Collection Report is a simple listing of the data sources acquired along with their locations, the procedures used to procure them and any difficulties encountered. This section will will aid both with future replication of this project and with the execution of similar future projects.</li>\n",
    "\n",
    "___Data Description Report___: The Data Description report describes the data that has been procured including its format, quantity: for example, the count of records and fields in each table, the characteristics of the “fields,” and any other surface characteristics discovered. Here is where I have evaluated whether or not the data acquired satisfies the analysis requirements.\n",
    "\n",
    "___Exploratory Data Analysis (EDA)___: EDA describes the results of exploring the data involved in this project. EDA includes the first findings and or initial hypothesis and their impact on the remainder of this project. I have also included graphs and plots to indicate data characteristics that suggest further examination and analysis of new data subsets.\n",
    "\n",
    "___Data Quality Report___: The Data Quality report lists the results of the data quality verification along with solutions vetted by subject matter experts to data quality.\n",
    "    \n",
    "__Note:__\n",
    "\n",
    "At this moment, the organization has no plans to procure external databases or invest in dispatching teams to each site as its engineers, analysts, and managers are busy managing the data they currently have. At some point, however, they might want to consider an extended deployment of data mining results, in which case purchasing additional IT infrastructure to capture sensor data not centrally registered may be quite useful. It may also be helpful to have demographic information to see how the maintainers across regions vary in skill set and maintenance practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initial Data Collection Report__\n",
    "\n",
    "    The dataset was put together by the Grouplens research group at the University of Minnesota. It comprises 1, 10, and 20 million ratings. And can be found at https://grouplens.org/datasets/movielens/. The following Data Collection Report is a simple listing of the data sources acquired along with their locations, the procedures used to procure them, and any difficulties encountered. This section will aid both with future replication of this project and with the execution of similar future projects.\n",
    "\n",
    "__Data Collection:__\n",
    "\n",
    "1.\tData Source: MovieLens (\n",
    "2.\tLocation: https://grouplens.org/datasets/movielens/\n",
    "3.\tMovie Ratings: Small: 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. Last updated 9/2018\n",
    "4.\tMethod: This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100836 ratings and 3683 tag applications across 9742 movies. These data were created by 610 users between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018.\n",
    "a.\tUsers were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "b.\tThe data are contained in the files links.csv, movies.csv, ratings.csv and tags.csv. More details about the contents and use of all these files follows.\n",
    "c.\tThis and other GroupLens data sets are publicly available for download at http://grouplens.org/datasets/.\n",
    "5.\tObstacles: This is a development dataset. As such, it may change over time and is not an appropriate dataset for shared research results. See available benchmark datasets if that is your intent.\n",
    "    \n",
    "    \n",
    "__Movie Ids__\n",
    "Only movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (e.g., id 1 corresponds to the URL https://movielens.org/movies/1). Movie ids are consistent between ratings.csv, tags.csv, movies.csv, and links.csv (i.e., the same id refers to the same movie across these four data files).\n",
    "\n",
    "__Ratings Data File Structure (ratings.csv)__\n",
    "All ratings are contained in the file ratings.csv. Each line of this file after the header row represents one rating of one movie by one user, and has the following format:\n",
    "\n",
    "userId,movieId,rating,timestamp\n",
    "The lines within this file are ordered first by userId, then, within user, by movieId.\n",
    "\n",
    "Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "\n",
    "Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# settings\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# data viz imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building scrapper: writing elegant parsing/scrapping function to parse data from internet\n",
    "def get_soup(url):\n",
    "    \"\"\"Constructs and returns a soup using the HTML content of `url` passed\"\"\"\n",
    "    # initialize a session\n",
    "    session = requests.Session()\n",
    "    # make the request\n",
    "    html = session.get(url)\n",
    "    # return the soup\n",
    "    return bs(html.content, \"html.parser\")\n",
    " \n",
    "def get_tables(soup):\n",
    "    \"\"\"Extracts and returns all tables in a soup object\"\"\"\n",
    "    return soup.find_all(\"table\",{\"id\":\"usa_table_countries_today\"})\n",
    "\n",
    "def get_table_headers(table):\n",
    "    \"\"\"Given a table soup, returns all the headers\"\"\"\n",
    "    headers = []\n",
    "    for th in table.find(\"tr\").find_all(\"th\"):\n",
    "        headers.append(th.text.strip())\n",
    "    return headers\n",
    " \n",
    "def get_table_rows(table):\n",
    "    \"\"\"Given a table, returns all its rows\"\"\"\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        cells = []\n",
    "        # grab all td tags in this table row\n",
    "        tds = tr.find_all(\"td\")\n",
    "        if len(tds) == 0:\n",
    "            # if no td tags, search for th tags\n",
    " \n",
    "            ths = tr.find_all(\"th\")\n",
    "            for th in ths:\n",
    "                cells.append(th.text.strip())\n",
    "        else:\n",
    "            # use regular td tags\n",
    "            for td in tds:\n",
    "                cells.append(td.text.strip())\n",
    "        rows.append(cells)\n",
    "    return rows\n",
    "\n",
    "def main(url):\n",
    "    # get the soup\n",
    "    soup = get_soup(url)\n",
    "    # extract all the tables from the web page\n",
    "    tables = get_tables(soup)\n",
    "    # iterate over all tables\n",
    "    for i, table in enumerate(tables, start=1):\n",
    "        # get the table headers\n",
    "        headers = get_table_headers(table)\n",
    "        # get all the rows of the table\n",
    "        rows = get_table_rows(table)\n",
    "        # save table as csv file\n",
    "        table_name = f\"table-{i}\"\n",
    "        save_as_csv(table_name, headers, rows)    \n",
    "\n",
    "def save_as_csv(table_name, headers, rows):\n",
    "    pd.DataFrame(rows, columns=headers).to_csv(\"data\\covid19_states.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data source: df1_covidcases\n",
    "\n",
    "# step1. instantiating scrapper function        \n",
    "main(\"https://www.worldometers.info/coronavirus/country/us/\")\n",
    "# step2. setting up column names: col_list\n",
    "col_list = [\"USAState\",\"TotalCases\",\"NewCases\",\"TotalDeaths\",\"NewDeaths\",\"ActiveCases\"]\n",
    "# step3. reading data in using read_csv, and casting col_list to headers: df1_covidcases\n",
    "df1_covidcases = pd.read_csv(\"data/covid19_states.csv\", usecols=col_list)\n",
    "\n",
    "\n",
    "# get data source: df2_humidity\n",
    "\n",
    "# step1. assign webpage address of interest that we will scrape\n",
    "url = 'http://www.usa.com/rank/us--average-humidity--state-rank.htm#:~:text=Rank%20Average%20Humidity%20%E2%96%BC%20State%20%2F%20Population%201.,4.%2080.76%25%20Maine%20%2F%201%2C328%2C535%2047%20more%20rows'\n",
    "# step2. use pandas read_html() tool to read and parse our site. The [0] indicates that we want to grab the first table on the webpage\n",
    "df2_humidity = pd.read_html(url)[0]\n",
    "# step3. set first row to headers\n",
    "df2_humidity = df2_humidity.rename(columns=df2_humidity.iloc[0])\n",
    "# step 4. drop duplicated header row inplace\n",
    "df2_humidity.drop(df2_humidity.index[0], inplace=True)\n",
    "# step 5. split data by the delimeter \"/\" into: 'State' and 'Population'\n",
    "df2_humidity[['State','Population']] = df2_humidity['State / Population'].str.split(\"/\",expand=True)\n",
    "# step 6. drop 'State / Population' column\n",
    "df2_humidity.drop(columns=['State / Population'], inplace=True)\n",
    "# step 7. rename columns using dictionary\n",
    "df2_humidity.rename(columns={'Average Humidity ▼' : 'Average Humidity', 'State' : 'USAState'},inplace=True)\n",
    "# step 8. write file to .csv\n",
    "df2_humidity.to_csv(\"data\\covid19_states_humidity.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        USAState TotalCases NewCases TotalDeaths  NewDeaths ActiveCases\n",
      "0      USA Total  3,359,838   +4,192     137,436       33.0   1,731,677\n",
      "1       New York    426,807      NaN      32,393        NaN     227,391\n",
      "2     California    319,985      NaN       7,030        4.0     227,398\n",
      "3          Texas    259,465      NaN       3,228        NaN     128,357\n",
      "4        Florida    254,511      NaN       4,197        NaN     218,244\n",
      "5     New Jersey    180,672      NaN      15,603        NaN      88,899\n",
      "6       Illinois    154,094      NaN       7,369        NaN      31,278\n",
      "7        Arizona    119,930      NaN       2,151        NaN     103,385\n",
      "8        Georgia    114,401      NaN       2,996        NaN      93,715\n",
      "9  Massachusetts    111,398      NaN       8,310        NaN       8,741\n",
      "   Rank Average Humidity        USAState   Population\n",
      "1    1.           82.01%           Iowa     3,078,116\n",
      "2    2.           81.86%  New Hampshire     1,321,069\n",
      "3    3.           81.46%         Alaska       728,300\n",
      "4    4.           80.76%          Maine     1,328,535\n",
      "5    5.           80.74%   North Dakota       704,925\n",
      "6    6.           80.61%      Minnesota     5,383,661\n",
      "7    7.           80.54%   South Dakota       834,708\n",
      "8    8.           80.40%        Montana     1,006,370\n",
      "9    9.           80.36%     California    38,066,920\n",
      "10  10.           79.71%       Colorado     5,197,580\n"
     ]
    }
   ],
   "source": [
    "# check data frames from scrapper: df1_covidcases & df2_humidity\n",
    "print(df1_covidcases.head())\n",
    "print(df2_humidity.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   USAState     64 non-null     object \n",
      " 1   TotalCases   64 non-null     object \n",
      " 2   NewCases     15 non-null     object \n",
      " 3   TotalDeaths  62 non-null     object \n",
      " 4   NewDeaths    9 non-null      float64\n",
      " 5   ActiveCases  64 non-null     object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 3.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# check the info for df1_covidcases\n",
    "# We see that there are some null values that we will need to work out. Aslo we may need to moralize values within each record for leading and lagging spaces.\n",
    "print(df1_covidcases.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 51 entries, 1 to 51\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Rank              51 non-null     object\n",
      " 1   Average Humidity  51 non-null     object\n",
      " 2   USAState          51 non-null     object\n",
      " 3   Population        51 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 2.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# check the info for df1_covidcases\n",
    "# We see that the values listed here are a lot more pure. To merge the dataframes we just need to esure primary and foreign keys are consistent on USAState\n",
    "print(df2_humidity.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USAState</th>\n",
       "      <th>TotalCases</th>\n",
       "      <th>NewCases</th>\n",
       "      <th>TotalDeaths</th>\n",
       "      <th>NewDeaths</th>\n",
       "      <th>ActiveCases</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Average Humidity</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.</td>\n",
       "      <td>82.01%</td>\n",
       "      <td>3,078,116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.</td>\n",
       "      <td>81.86%</td>\n",
       "      <td>1,321,069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.</td>\n",
       "      <td>81.46%</td>\n",
       "      <td>728,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.</td>\n",
       "      <td>80.76%</td>\n",
       "      <td>1,328,535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.</td>\n",
       "      <td>80.74%</td>\n",
       "      <td>704,925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         USAState TotalCases NewCases TotalDeaths  NewDeaths ActiveCases Rank Average Humidity  Population\n",
       "0           Iowa         NaN      NaN         NaN        NaN         NaN   1.           82.01%   3,078,116\n",
       "1  New Hampshire         NaN      NaN         NaN        NaN         NaN   2.           81.86%   1,321,069\n",
       "2         Alaska         NaN      NaN         NaN        NaN         NaN   3.           81.46%     728,300\n",
       "3          Maine         NaN      NaN         NaN        NaN         NaN   4.           80.76%   1,328,535\n",
       "4   North Dakota         NaN      NaN         NaN        NaN         NaN   5.           80.74%     704,925"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge wrangled dataframes\n",
    "# we see that there is an issue with the primary and foreign keys i.e. lagging space or even spelling\n",
    "test_merge = pd.merge(df1_covidcases,df2_humidity, how='right', on='USAState')\n",
    "test_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19                         Alabama\n",
       "36                        Alabama \n",
       "49                          Alaska\n",
       "3                          Alaska \n",
       "7                          Arizona\n",
       "14                        Arizona \n",
       "30                        Arkansas\n",
       "31                       Arkansas \n",
       "2                       California\n",
       "9                      California \n",
       "24                        Colorado\n",
       "10                       Colorado \n",
       "21                     Connecticut\n",
       "16                    Connecticut \n",
       "38                        Delaware\n",
       "50                       Delaware \n",
       "62           Diamond Princess Ship\n",
       "40            District Of Columbia\n",
       "24           District of Columbia \n",
       "58                 Federal Prisons\n",
       "4                          Florida\n",
       "28                        Florida \n",
       "8                          Georgia\n",
       "39                        Georgia \n",
       "60             Grand Princess Ship\n",
       "52                            Guam\n",
       "51                          Hawaii\n",
       "47                         Hawaii \n",
       "41                           Idaho\n",
       "12                          Idaho \n",
       "6                         Illinois\n",
       "30                       Illinois \n",
       "20                         Indiana\n",
       "38                        Indiana \n",
       "27                            Iowa\n",
       "1                            Iowa \n",
       "35                          Kansas\n",
       "15                         Kansas \n",
       "34                        Kentucky\n",
       "37                       Kentucky \n",
       "12                       Louisiana\n",
       "41                      Louisiana \n",
       "46                           Maine\n",
       "4                           Maine \n",
       "14                        Maryland\n",
       "49                       Maryland \n",
       "9                    Massachusetts\n",
       "44                  Massachusetts \n",
       "13                        Michigan\n",
       "46                       Michigan \n",
       "22                       Minnesota\n",
       "6                       Minnesota \n",
       "26                     Mississippi\n",
       "40                    Mississippi \n",
       "29                        Missouri\n",
       "22                       Missouri \n",
       "48                         Montana\n",
       "8                         Montana \n",
       "59                   Navajo Nation\n",
       "32                        Nebraska\n",
       "18                       Nebraska \n",
       "31                          Nevada\n",
       "20                         Nevada \n",
       "43                   New Hampshire\n",
       "2                   New Hampshire \n",
       "5                       New Jersey\n",
       "51                     New Jersey \n",
       "37                      New Mexico\n",
       "35                     New Mexico \n",
       "1                         New York\n",
       "42                       New York \n",
       "11                  North Carolina\n",
       "27                 North Carolina \n",
       "44                    North Dakota\n",
       "5                    North Dakota \n",
       "53        Northern Mariana Islands\n",
       "16                            Ohio\n",
       "23                           Ohio \n",
       "33                        Oklahoma\n",
       "34                       Oklahoma \n",
       "39                          Oregon\n",
       "11                         Oregon \n",
       "10                    Pennsylvania\n",
       "21                   Pennsylvania \n",
       "54                     Puerto Rico\n",
       "36                    Rhode Island\n",
       "25                   Rhode Island \n",
       "18                  South Carolina\n",
       "48                 South Carolina \n",
       "42                    South Dakota\n",
       "7                    South Dakota \n",
       "17                       Tennessee\n",
       "43                      Tennessee \n",
       "3                            Texas\n",
       "29                          Texas \n",
       "63                          Total:\n",
       "57                     US Military\n",
       "0                        USA Total\n",
       "55    United States Virgin Islands\n",
       "28                            Utah\n",
       "45                           Utah \n",
       "50                         Vermont\n",
       "32                        Vermont \n",
       "56                 Veteran Affairs\n",
       "15                        Virginia\n",
       "33                       Virginia \n",
       "23                      Washington\n",
       "17                     Washington \n",
       "45                   West Virginia\n",
       "19                  West Virginia \n",
       "25                       Wisconsin\n",
       "26                      Wisconsin \n",
       "61               Wuhan Repatriated\n",
       "47                         Wyoming\n",
       "13                        Wyoming \n",
       "Name: USAState, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we look at value fields for primary keys to see why they are off. We will need to address this in the data preparation.\n",
    "frames = [df1_covidcases['USAState'], df2_humidity['USAState']]\n",
    "result = pd.concat(frames).sort_values()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to .csv\n",
    "# write out covid19_states.csv\n",
    "df1_covidcases.to_csv(\"data\\covid19_states_cases.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "# write out covid19_states_humidity.csv\n",
    "df2_humidity.to_csv(\"data\\covid19_states_humidity.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
